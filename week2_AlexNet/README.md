# Week 2: AlexNet ë…¼ë¬¸ êµ¬í˜„ ë° CIFARâ€‘10 ì‹¤í—˜

> **ImageNet Classification with Deep Convolutional Neural Networks**  
> Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton (2012)

---

## ğŸ§ª ì‹¤í—˜ ëª©í‘œ

- ILSVRC 2012 ìš°ìŠ¹ ëª¨ë¸ **AlexNet**ì„ ê¸°ë°˜ìœ¼ë¡œ CIFARâ€‘10 ë°ì´í„°ì— ëŒ€í•´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ êµ¬í˜„
- ë…¼ë¬¸ í•µì‹¬ ìš”ì†Œ(**LRN, ReLU, Overlapping Pooling, Dropout** ë“±)ë¥¼ ì§ì ‘ êµ¬í˜„í•´ ë™ì‘ ë°©ì‹ ì´í•´
- PCA ê¸°ë°˜ Color Augmentation(Lighting)ì„ í¬í•¨í•˜ì—¬ ë…¼ë¬¸ì˜ ë°ì´í„° ì¦ê°• ì¼ë¶€ ì¬í˜„
- Conv1 í•„í„° ì‹œê°í™” ë° FC feature ê¸°ë°˜ ìœ ì‚¬ ì´ë¯¸ì§€ ê²€ìƒ‰ìœ¼ë¡œ í‘œí˜„ í•™ìŠµ íŠ¹ì„± ë¶„ì„

---

## ğŸ“ íŒŒì¼ êµ¬ì„±

| íŒŒì¼ëª…             | ì„¤ëª…                                                 |
| --------------- | -------------------------------------------------- |
| `train.py`      | AlexNet í•™ìŠµ/ê²€ì¦ ì „ì²´ íŒŒì´í”„ë¼ì¸                             |
| `alexnet.py`    | AlexNet êµ¬ì¡° ì •ì˜ (LRN, Dropout í¬í•¨)                    |
| `dataloader.py` | CIFARâ€‘10 ë°ì´í„° ë¡œë” + PCA Color Augmentation(Lighting) |
| `metrics.py`    | Topâ€‘1 / Topâ€‘5 accuracy ê³„ì‚°                          |
| `visualize.py`  | Conv1 í•„í„° ì‹œê°í™”, Nearest Neighbor ê²€ìƒ‰ ê¸°ëŠ¥               |

---

## âœ… ì£¼ìš” êµ¬í˜„ ë‚´ìš©

- [x] AlexNet ì „ì²´ êµ¬ì¡° êµ¬í˜„ (ë…¼ë¬¸ Section 3 ê¸°ë°˜)
- [x] Local Response Normalization (LRN) êµ¬í˜„
- [x] Overlapping Max Pooling (stride < kernel)
- [x] Dropout ì ìš©í•œ Fullyâ€‘Connected êµ¬ì¡° ì¬í˜„
- [x] PCA ê¸°ë°˜ Color Augmentation (Lighting) êµ¬í˜„
- [x] Conv1 í•„í„° ì‹œê°í™”
- [x] FC feature ê¸°ë°˜ ìœ ì‚¬ ì´ë¯¸ì§€ ê²€ìƒ‰ (Nearest Neighbor)

---

## â–¶ï¸ ì‹¤í–‰ ë°©ë²•

```bash
# AlexNet í•™ìŠµ
python train.py

âš ï¸ CPU í™˜ê²½ì—ì„œëŠ” AlexNetì´ ë§¤ìš° ëŠë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ epoch=10, batch_size=128, í˜¹ì€ ì„œë¸Œì…‹ ë°ì´í„° ëª¨ë“œë¥¼ ì¶”ì²œ
```

---

## ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ìš”ì•½

ì´ë²ˆ ì‹¤í—˜ì€ **AlexNet**ì„ **CIFAR-10 ë°ì´í„°ì…‹ (Subset 5ì²œ ê°œ)** ê¸°ì¤€ìœ¼ë¡œ **10 Epoch** í•™ìŠµí•˜ê³ ,  
Conv1 í•„í„°ë¥¼ ì‹œê°í™”í•œ ê²°ê³¼ë¥¼ ì¬í˜„í•œ ê²ƒì´ë‹¤.

| í•­ëª© | ê²°ê³¼ |
|------|------|
| Epoch ìˆ˜ | 10 |
| Train Accuracy (ìµœì¢…) | 25.82% |
| Test Top-1 Accuracy | 26.70% |
| Test Top-5 Accuracy | 81.30% |
| Conv1 í•„í„° ì‹œê°í™” | ì •ìƒ ì¶œë ¥ë¨ (96ê°œ í•„í„°) |

```bash
ğŸš€ [Start] Training begins...
ğŸ–¥ï¸ Using device: cpu
ğŸ“¦ Loading data...
âœ… Data loaded!

ğŸ“š [Epoch 1/10] ------------------------------
  ğŸ”„ Batch 1/40 | Loss: 2.3020
  ğŸ”„ Batch 6/40 | Loss: 2.3020
  ğŸ”„ Batch 11/40 | Loss: 2.3032
  ğŸ”„ Batch 16/40 | Loss: 2.3024
  ğŸ”„ Batch 21/40 | Loss: 2.3048
  ğŸ”„ Batch 26/40 | Loss: 2.3017
  ğŸ”„ Batch 31/40 | Loss: 2.3029
  ğŸ”„ Batch 36/40 | Loss: 2.3020
ğŸ“Š Epoch 1 Summary: Loss = 92.106, Accuracy = 10.36%

ğŸ“š [Epoch 2/10] ------------------------------
  ğŸ”„ Batch 1/40 | Loss: 2.3039
  ğŸ”„ Batch 6/40 | Loss: 2.3024
  ğŸ”„ Batch 11/40 | Loss: 2.3024
  ğŸ”„ Batch 16/40 | Loss: 2.3008
  ğŸ”„ Batch 21/40 | Loss: 2.3033
  ğŸ”„ Batch 26/40 | Loss: 2.3055
  ğŸ”„ Batch 31/40 | Loss: 2.2993
  ğŸ”„ Batch 36/40 | Loss: 2.3072
ğŸ“Š Epoch 2 Summary: Loss = 92.071, Accuracy = 10.50%

ğŸ“š [Epoch 3/10] ------------------------------
  ğŸ”„ Batch 1/40 | Loss: 2.3033
  ğŸ”„ Batch 6/40 | Loss: 2.3005
  ğŸ”„ Batch 11/40 | Loss: 2.2972
  ğŸ”„ Batch 16/40 | Loss: 2.3041
  ğŸ”„ Batch 21/40 | Loss: 2.3052
  ğŸ”„ Batch 26/40 | Loss: 2.3022
  ğŸ”„ Batch 31/40 | Loss: 2.3028
  ğŸ”„ Batch 36/40 | Loss: 2.3000
ğŸ“Š Epoch 3 Summary: Loss = 92.068, Accuracy = 10.90%

ğŸ“š [Epoch 4/10] ------------------------------
  ğŸ”„ Batch 1/40 | Loss: 2.2936
  ğŸ”„ Batch 6/40 | Loss: 2.3068
  ğŸ”„ Batch 11/40 | Loss: 2.3027
  ğŸ”„ Batch 16/40 | Loss: 2.3038
  ğŸ”„ Batch 21/40 | Loss: 2.3008
  ğŸ”„ Batch 26/40 | Loss: 2.3007
  ğŸ”„ Batch 31/40 | Loss: 2.3002
  ğŸ”„ Batch 36/40 | Loss: 2.3001
ğŸ“Š Epoch 4 Summary: Loss = 92.000, Accuracy = 12.98%

ğŸ“š [Epoch 5/10] ------------------------------
  ğŸ”„ Batch 1/40 | Loss: 2.3002
  ğŸ”„ Batch 6/40 | Loss: 2.3016
  ğŸ”„ Batch 11/40 | Loss: 2.2954
  ğŸ”„ Batch 16/40 | Loss: 2.2917
  ğŸ”„ Batch 21/40 | Loss: 2.2920
  ğŸ”„ Batch 26/40 | Loss: 2.2955
  ğŸ”„ Batch 31/40 | Loss: 2.2872
  ğŸ”„ Batch 36/40 | Loss: 2.2966
ğŸ“Š Epoch 5 Summary: Loss = 91.662, Accuracy = 13.38%

ğŸ“š [Epoch 6/10] ------------------------------
  ğŸ”„ Batch 1/40 | Loss: 2.2610
  ğŸ”„ Batch 6/40 | Loss: 2.2653
  ğŸ”„ Batch 11/40 | Loss: 2.2556
  ğŸ”„ Batch 16/40 | Loss: 2.2337
  ğŸ”„ Batch 21/40 | Loss: 2.2449
  ğŸ”„ Batch 26/40 | Loss: 2.2313
  ğŸ”„ Batch 31/40 | Loss: 2.2090
  ğŸ”„ Batch 36/40 | Loss: 2.1690
ğŸ“Š Epoch 6 Summary: Loss = 89.078, Accuracy = 14.40%

ğŸ“š [Epoch 7/10] ------------------------------
  ğŸ”„ Batch 1/40 | Loss: 2.1276
  ğŸ”„ Batch 6/40 | Loss: 2.1064
  ğŸ”„ Batch 11/40 | Loss: 2.1385
  ğŸ”„ Batch 16/40 | Loss: 2.0993
  ğŸ”„ Batch 21/40 | Loss: 2.1172
  ğŸ”„ Batch 26/40 | Loss: 2.0983
  ğŸ”„ Batch 31/40 | Loss: 2.0720
  ğŸ”„ Batch 36/40 | Loss: 2.1556
ğŸ“Š Epoch 7 Summary: Loss = 85.073, Accuracy = 22.14%

ğŸ“š [Epoch 8/10] ------------------------------
  ğŸ”„ Batch 1/40 | Loss: 2.1246
  ğŸ”„ Batch 6/40 | Loss: 2.1395
  ğŸ”„ Batch 11/40 | Loss: 2.1510
  ğŸ”„ Batch 16/40 | Loss: 2.1531
  ğŸ”„ Batch 21/40 | Loss: 2.0998
  ğŸ”„ Batch 26/40 | Loss: 2.0598
  ğŸ”„ Batch 31/40 | Loss: 2.1698
  ğŸ”„ Batch 36/40 | Loss: 2.1197
ğŸ“Š Epoch 8 Summary: Loss = 83.934, Accuracy = 23.46%

ğŸ“š [Epoch 9/10] ------------------------------
  ğŸ”„ Batch 1/40 | Loss: 2.1025
  ğŸ”„ Batch 6/40 | Loss: 2.0536
  ğŸ”„ Batch 11/40 | Loss: 2.0271
  ğŸ”„ Batch 16/40 | Loss: 2.1097
  ğŸ”„ Batch 21/40 | Loss: 2.0795
  ğŸ”„ Batch 26/40 | Loss: 2.0356
  ğŸ”„ Batch 31/40 | Loss: 2.0720
  ğŸ”„ Batch 36/40 | Loss: 1.9891
ğŸ“Š Epoch 9 Summary: Loss = 82.367, Accuracy = 23.42%

ğŸ“š [Epoch 10/10] ------------------------------
  ğŸ”„ Batch 1/40 | Loss: 1.9900
  ğŸ”„ Batch 6/40 | Loss: 1.9712
  ğŸ”„ Batch 11/40 | Loss: 1.9378
  ğŸ”„ Batch 16/40 | Loss: 1.9652
  ğŸ”„ Batch 21/40 | Loss: 1.9481
  ğŸ”„ Batch 26/40 | Loss: 2.0471
  ğŸ”„ Batch 31/40 | Loss: 1.9105
  ğŸ”„ Batch 36/40 | Loss: 1.9957
ğŸ“Š Epoch 10 Summary: Loss = 80.209, Accuracy = 25.82%

ğŸ§ª Running evaluation on test set...
âœ… [Test Results] Top-1 Accuracy: 26.70%, Top-5 Accuracy: 81.30%
ğŸ‰ [Done] Training complete!
ğŸ–¼ï¸ Visualizing Conv1 filters...
```

---

### 1. í•™ìŠµ ì§„í–‰ í•´ì„

#### ì •í™•ë„ ìƒìŠ¹ ì¶”ì´
- Epoch 1~3: ì•½ 10\~13% â†’ ëœë¤ ì¶”ì¸¡ ìˆ˜ì¤€
- Epoch 4~6: ì†ì‹¤ ê°ì†Œ + ì •í™•ë„ ì ì§„ì  ì¦ê°€ (14\~22%)
- Epoch 10 : Train 25.82%, Test 26.70%
- Overfittingì€ ì•„ì§ ì—†ìŒ â†’ train/test ë¹„ìŠ·í•œ ì„±ëŠ¥

#### ì†ì‹¤ ê°ì†Œ
- Epoch 1 : `Loss â‰ˆ 92.1` â†’ Epoch 10: `Loss â‰ˆ 80.2`
- Optimizer : SGD + Momentum
- `epoch=30, 60`ë¶€í„° í•™ìŠµë¥  ê°ì†Œ ì˜ˆì •, í˜„ì¬ëŠ” ì ìš© ì „

### 2. Conv1 í•„í„° ì‹œê°í™” í•´ì„
![img.png](pictures_for_README/img.png)

| íŒ¨í„´ ìœ í˜• | ì˜ë¯¸ |
|-----------|------|
| ğŸ¨ ì•Œë¡ë‹¬ë¡í•œ ìƒ‰ìƒ ë¸”ë¡­ | ìƒ‰ìƒ ì°¨ì´ ê°ì§€ (RGB ì¡°í•© êµ¬ë¶„) |
| â¬› ì²´í¬ë¬´ëŠ¬/ì¤„ë¬´ëŠ¬ | í…ìŠ¤ì²˜/ê°„ë‹¨í•œ ì—£ì§€ ê°ì§€ í•„í„° ì´ˆê¸° í˜•íƒœ |
| âŒ ë¬´ì‘ìœ„ ëŠë‚Œ | í•™ìŠµ ì´ˆê¸°ì— ê°€ê¹Œì›Œ ì•„ì§ ëœ í•™ìŠµëœ ìƒíƒœ |

- Conv1 í•„í„°ëŠ” **11Ã—11Ã—3** êµ¬ì¡°ë¡œ êµ¬ì„±
- ëª¨ë¸ì´ ê°€ì¥ ë¨¼ì € ì‚¬ìš©í•˜ëŠ” ì‹œê° feature detector
- í˜„ì¬ëŠ” ìƒ‰ìƒ ìœ„ì£¼ì˜ ëœë¤ í•„í„°ì— ê°€ê¹Œì›€ â†’ ë” í•™ìŠµí•˜ë©´ Gabor-like ì—£ì§€ í•„í„°ë¡œ ì§„í™”

> ğŸ§  Conv1ë§Œ ì‹œê°í™”í•˜ëŠ” ì´ìœ ?  
> í•„í„° í¬ê¸°ì™€ ì±„ë„ ìˆ˜ê°€ ì‘ì•„ ì´ë¯¸ì§€ì²˜ëŸ¼ ì§ì ‘ ì‹œê°í™”ê°€ ê°€ëŠ¥í•˜ë©°, Conv2 ì´ìƒì€ ì±„ë„ì´ ë§ì•„ ì‹œê°ì ìœ¼ë¡œ í•´ì„í•˜ê¸° ì–´ë ¤ì›€

### 3. Test Accuracy ë¶„ì„

| ë©”íŠ¸ë¦­ | ì˜ë¯¸ | ê°’ |
|--------|------|----|
| Top-1 Accuracy | ì •í™•íˆ ì •ë‹µ class ì˜ˆì¸¡ | **26.70%** |
| Top-5 Accuracy | ìƒìœ„ 5ê°œ ì˜ˆì¸¡ ë‚´ ì •ë‹µ í¬í•¨ | **81.30%** |

- Top-5ê°€ ë§¤ìš° ë†’ë‹¤ëŠ” ê²ƒì€ ëª¨ë¸ì´ feature space êµ¬ì¡°ë¥¼ ì˜ í˜•ì„±í•˜ê³  ìˆë‹¤ëŠ” ì‹ í˜¸

---

## ğŸ“š ì°¸ê³  ìë£Œ

- ğŸ“„ [ë…¼ë¬¸ ì›ë¬¸ (NIPS 2012)](https://proceedings.neurips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)
- ğŸ“˜ [PyTorch AlexNet ê³µì‹ êµ¬í˜„](https://pytorch.org/vision/stable/models/generated/torchvision.models.alexnet.html)
- ğŸ› ï¸ [ImageNet Challenge (ILSVRC) ì†Œê°œ](http://image-net.org/challenges/LSVRC/)
- ğŸ“¦ [CIFAR-10 ë°ì´í„°ì…‹ ì„¤ëª…](https://www.cs.toronto.edu/~kriz/cifar.html)
